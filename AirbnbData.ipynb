{"cells":[{"cell_type":"markdown","metadata":{"id":"rqedyIKfM-KB"},"source":["\n","# Data loading"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":26548,"status":"ok","timestamp":1716727495060,"user":{"displayName":"Stefano Andreotti","userId":"07616716346074695388"},"user_tz":-120},"id":"JEtKAtUrMlzz","outputId":"99f3f2fa-bcc8-46af-ef8d-53c3583490a0"},"outputs":[],"source":["import numpy as np # linear algebra\n","import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n","import seaborn as sns\n","import matplotlib.pyplot as plt\n","import warnings\n","import numpy.random as rand\n","from scipy import stats\n","from sklearn import preprocessing \n","\n","import re\n","import json\n","\n","from sklearn.model_selection import train_test_split, GridSearchCV\n","from sklearn.ensemble import RandomForestClassifier\n","from sklearn.metrics import classification_report, confusion_matrix\n","from sklearn import metrics\n","from AutoClean import AutoClean\n","\n","warnings.filterwarnings('ignore')"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":354,"status":"ok","timestamp":1716729226124,"user":{"displayName":"Stefano Andreotti","userId":"07616716346074695388"},"user_tz":-120},"id":"b3-06VkqALz7","outputId":"8a589d82-8d40-428e-8e9b-2e28dc5253d7"},"outputs":[],"source":["data=pd.read_csv('./source_data/airbnb.csv')\n","\n","data.info()"]},{"cell_type":"markdown","metadata":{},"source":["# Data exploration"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["# Check distribution of target variable\n","\n","unos=0\n","zeros=0\n","for i in range(len(data['Rating'])):\n","    if data['Rating'][i] == 'Y':\n","        unos+=1\n","    else:\n","        zeros+=1\n","\n","sns.countplot(x = data['Rating'])\n","print(f\"N={zeros} - Y={unos}\")\n","plt.show()"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":355},"executionInfo":{"elapsed":507,"status":"ok","timestamp":1716727503763,"user":{"displayName":"Stefano Andreotti","userId":"07616716346074695388"},"user_tz":-120},"id":"JU9frQdu_S91","outputId":"866de5c4-b496-44ff-8e39-5fd1f72d3d5b"},"outputs":[],"source":["plt.figure(figsize=(15,8), dpi =500)\n","sns.heatmap(data.corr(method='pearson'),annot=True,fmt=\".2f\", linewidth=.5)\n","plt.show()"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":226,"status":"ok","timestamp":1716727507168,"user":{"displayName":"Stefano Andreotti","userId":"07616716346074695388"},"user_tz":-120},"id":"PDQaZU96_5PW","outputId":"051bbfa9-63da-48e5-9ef4-dbd6853baf9d"},"outputs":[],"source":["data.isnull().sum()"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["new_columns = []\n","for column in data.columns:\n","    new_col = re.sub(\"[()%=.-/]\", '', column)\n","    new_col = new_col.replace(' ', '_')\n","    if new_col[-1] == '_':\n","        new_col = new_col[:-1]\n","\n","    #print(column + \" -> \" + new_col)\n","    data.rename(columns={column: new_col}, inplace=True)\n","\n","# replacing values\n","label_encoder = preprocessing.LabelEncoder() \n","data['Rating']= label_encoder.fit_transform(data['Rating']) \n","data['LocationName']= label_encoder.fit_transform(data['LocationName']) \n","\n","data.info()\n","data.to_csv('./airbnb_trim.csv', index=False, sep=',')"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["# Split data into training and testing sets\n","X = data.drop(\"Rating\", axis=1)  # Features\n","y = data[\"Rating\"]  # Target variable"]},{"cell_type":"markdown","metadata":{"id":"iDk_Z6bH_sfy"},"source":["# Starting Model"]},{"cell_type":"code","execution_count":null,"metadata":{"executionInfo":{"elapsed":6,"status":"ok","timestamp":1716727513979,"user":{"displayName":"Stefano Andreotti","userId":"07616716346074695388"},"user_tz":-120},"id":"c4q7wvkm_sDE"},"outputs":[],"source":["# Splitting dataset\n","X_train, X_test, y_train, y_test = \\\n","    train_test_split(X, y, test_size=0.2)"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":75},"executionInfo":{"elapsed":358,"status":"ok","timestamp":1716727519159,"user":{"displayName":"Stefano Andreotti","userId":"07616716346074695388"},"user_tz":-120},"id":"j6y_iXyWAiM8","outputId":"aeb65f32-2976-4429-f1a0-9d65f1f5de25"},"outputs":[],"source":["# Define the parameter grid for GridSearchCV\n","param_grid = {\n","    \"n_estimators\": [50, 100, 200],  # Number of trees\n","    \"max_depth\": [4, 8, 16],  # Maximum depth of each tree\n","    \"min_samples_split\": [2, 5, 10],  # Minimum number of samples to split a node\n","    \"min_samples_leaf\": [1, 2, 4],  # Minimum number of samples in each leaf\n","    \"max_features\": [\"sqrt\", \"log2\"],\n","    \"random_state\": [0, 100, 1000]\n","}\n","\n","# Create the GridSearchCV object\n","model = GridSearchCV(RandomForestClassifier(), param_grid, cv=5, scoring=\"accuracy\")\n","\n","# Fit the model to the training data\n","model.fit(X_train, y_train)\n","\n","# Get the best hyperparameters and best score\n","best_params = model.best_params_\n","best_score = model.best_score_\n","\n","\n","print(\"Best Hyperparameters:\", best_params)\n","print(\"Best Score:\", best_score)"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["# Building Model\n","best_params = {'max_depth': 4, 'max_features': 'sqrt', 'min_samples_leaf': 1, 'min_samples_split': 2, 'n_estimators': 100, 'random_state': 1000}\n","clf = RandomForestClassifier(**best_params)\n","\n","# Training the model on the training dataset\n","clf.fit(X_train, y_train)\n","\n","# performing predictions\n","y_pred = clf.predict(X_test)\n","accuracy=metrics.accuracy_score(y_test,y_pred)\n","report=classification_report(y_test,y_pred)\n","\n","print(\"Classification Report:\")\n","print(report)"]},{"cell_type":"markdown","metadata":{"id":"A3x0KvS_79GV"},"source":["# Make data dirty"]},{"cell_type":"markdown","metadata":{},"source":["### Adding Outliers Values"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["def add_outliers(df, outlier_percent):\n","    \"\"\"\n","    Changes a specified percentage of values in a DataFrame to values outside the \n","    existing feature range.\n","\n","    Args:\n","        df (pandas.DataFrame): The input DataFrame.\n","        outlier_percent (float): The percentage of values to change (0.0 to 1.0).\n","\n","    Returns:\n","        pandas.DataFrame: The modified DataFrame with outliers introduced.\n","    \"\"\"\n","\n","    if outlier_percent < 0 or outlier_percent > 1:\n","        raise ValueError(\"outlier_percent must be between 0 and 1\")\n","\n","    # Create a mask with True for values to change\n","    outlier_mask = rand.rand(df.shape[0], df.shape[1]) < outlier_percent\n","    df_outliers = df.copy()\n","\n","    for col in df.columns:\n","        # Get minimum and maximum values (excluding potential existing outliers)\n","        min_val = df.min(axis=0)[col]\n","        max_val = df.max(axis=0)[col]\n","\n","        # Generate random values outside the range for outliers\n","        df_outliers[col] = [rand.uniform((min_val*0.3)-df.loc[i, col], (max_val*0.3)-df.loc[i, col]) for i in range(df.shape[0])]\n","\n","    \n","    df_outliers = df_outliers * outlier_mask\n","\n","    return df + df_outliers"]},{"cell_type":"markdown","metadata":{},"source":["### Change distribution"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["def add_gaussian_noise(df, noise_percent):\n","  \"\"\"\n","  Adds Gaussian noise to specified columns in a DataFrame for a given percentage.\n","\n","  Args:\n","      df (pandas.DataFrame): The input DataFrame.\n","      noise_percent (float): The percentage of standard deviation to add as noise (0.0 to 1.0).\n","      columns (list): A list of column names to add noise to.\n","\n","  Returns:\n","      pandas.DataFrame: The modified DataFrame with Gaussian noise added.\n","  \"\"\"\n","\n","  if noise_percent < 0 or noise_percent > 1:\n","    raise ValueError(\"noise_percent must be between 0.0 and 1.0\")\n","\n","# Create a mask with True for values to change\n","  outlier_mask = rand.rand(df.shape[0], df.shape[1]) < noise_percent\n","  df_gauss = df.copy()\n","\n","  for col in df_gauss.columns:\n","    # Get standard deviation for noise based on percentage\n","    std_dev = df_gauss[col].std() * noise_percent\n","\n","\n","    gaussian_noise = np.random.normal(0, std_dev, df_gauss.shape[0])\n","    df_gauss[col] = df_gauss[col] + gaussian_noise\n","\n","  \n","  df_gauss = df_gauss * outlier_mask\n","\n","  return df + df_gauss\n"]},{"cell_type":"markdown","metadata":{},"source":["### Adding Missing Values"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["def add_missing_values(df, missing_percent):\n","    \"\"\"\n","    Adds missing values randomly to a DataFrame for a specified percentage.\n","\n","    Args:\n","        df (pandas.DataFrame): The input DataFrame.\n","        missing_percent (float): The percentage of missing values to add (0.0 to 1.0).\n","\n","    Returns:\n","        pandas.DataFrame: The modified DataFrame with missing values added.\n","    \"\"\"\n","\n","    if missing_percent < 0 or missing_percent > 1:\n","        raise ValueError(\"missing_percent must be between 0 and 1\")\n","\n","    # Create a mask with True for missing values based on the percentage\n","    missing_mask = np.random.rand(df.shape[0], df.shape[1]) < missing_percent\n","\n","    # Replace existing values with NaN based on the mask\n","    df_with_missing = df.where(~missing_mask, np.NAN)\n","\n","    return df_with_missing\n"]},{"cell_type":"markdown","metadata":{},"source":["### Creating dirty data"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["# 10, 20, 30 percent of data missing\n","dirty_data_10 = add_outliers(data.copy(), 0.04)\n","dirty_data_10 = add_gaussian_noise(dirty_data_10, 0.03)\n","dirty_data_10 = add_missing_values(dirty_data_10, 0.04)\n","\n","dirty_data_20 = add_outliers(data.copy(), 0.07)\n","dirty_data_20 = add_gaussian_noise(dirty_data_20, 0.07)\n","dirty_data_20 = add_missing_values(dirty_data_20, 0.08)\n","\n","dirty_data_30 = add_outliers(data.copy(), 0.12)\n","dirty_data_30 = add_gaussian_noise(dirty_data_30, 0.1)\n","dirty_data_30 = add_missing_values(dirty_data_30, 0.12)\n","\n","# Salving\n","dirty_data_10.to_csv('./dirty_data/dirty_data_10.csv', index=False, sep=',')\n","dirty_data_20.to_csv('./dirty_data/dirty_data_20.csv', index=False, sep=',')\n","dirty_data_30.to_csv('./dirty_data/dirty_data_30.csv', index=False, sep=',')"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["differences = data.count().sum() - (data == dirty_data_10).astype(int).sum().sum()\n","total_val = data.shape[0]*data.shape[1]\n","print(\"dirty_data_10:\")\n","print(f\"Different values: {differences} out of {total_val}\")\n","print(f\"Percentage: {round((differences*100)/total_val, 2)}%\")\n","print()\n","\n","differences = data.count().sum() - (data == dirty_data_20).astype(int).sum().sum()\n","print(\"dirty_data_20:\")\n","print(f\"Different values: {differences} out of {total_val}\")\n","print(f\"Percentage: {round((differences*100)/total_val, 2)}%\")\n","print()\n","\n","differences = data.count().sum() - (data == dirty_data_30).astype(int).sum().sum()\n","print(\"dirty_data_30:\")\n","print(f\"Different values: {differences} out of {total_val}\")\n","print(f\"Percentage: {round((differences*100)/total_val, 2)}%\")\n","print()"]},{"cell_type":"markdown","metadata":{"id":"CtqNbcqF7_Mu"},"source":["# Cleaning data"]},{"cell_type":"markdown","metadata":{},"source":["## Manual cleaning"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["# handle missing values\n","def remove_missing(dirty_data, MODE='mean'):\n","    print(f\"Null data before:  {dirty_data.isna().any().sum()}\")\n","\n","    if dirty_data.isna().any().sum() > 0:\n","        if MODE == 'mean':\n","            for column in dirty_data.columns:\n","                if dirty_data[column].isnull().values.any():\n","                    dirty_data[column].fillna(float(dirty_data[column].mean()), inplace=True)\n","        elif MODE == 'delete':\n","            dirty_data.dropna(inplace=True)\n","        elif MODE == 'zeros':\n","            dirty_data.fillna(0, inplace=True)\n","\n","        print(f\"Null data after:  {dirty_data.isna().any().sum()}\")"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["# remove outliers\n","def remove_outliers(dirty_data, threshold_z=2):\n","    print(\"Original Dataframe shape:\", dirty_data.shape)\n","    for column in dirty_data.columns:\n","        z = np.abs(stats.zscore(dirty_data[column]))\n","\n","        outlier_indices = np.where(z > threshold_z)[0]\n","        no_outliers = dirty_data.drop(outlier_indices)\n","    \n","    print(\"Dataframe shape after removing outliers:\", no_outliers.shape)\n"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["def encode_y(data):\n","    label_encoder = preprocessing.LabelEncoder() \n","    data['Rating']= label_encoder.fit_transform(data['Rating']) \n","\n","    y = data['Rating']\n","    y_mean = data['Rating'].mean()\n","    for i in range(len(y)):\n","        if y[i] >= y_mean:\n","            y[i]=1\n","        else:\n","            y[i]=0\n","\n","    return y"]},{"cell_type":"markdown","metadata":{},"source":["### 10%"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["dirty_data = pd.read_csv('./dirty_data/dirty_data_10.csv')\n","dirty_data.info()"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["# Check distribution of target variable\n","y = dirty_data['Rating']\n","y_mean = dirty_data['Rating'].mean()\n","unos=0\n","zeros=0\n","for i in range(len(y)):\n","    if y[i] >= y_mean:\n","        y[i]=1\n","        unos+=1\n","    else:\n","        y[i]=0\n","        zeros+=1\n","\n","\n","sns.countplot(x = y)\n","print(f\"N={zeros} - Y={unos}\")\n","plt.show()"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["remove_missing(dirty_data)\n","remove_outliers(dirty_data)"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["# Split data into training and testing sets\n","X = dirty_data.drop(\"Rating\", axis=1)  # Features\n","y = encode_y(dirty_data)\n","\n","# Splitting dataset\n","X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2)\n","\n","# Building Model\n","best_params = {'max_depth': 4, 'max_features': 'sqrt', 'min_samples_leaf': 1, 'min_samples_split': 2, 'n_estimators': 100, 'random_state': 1000}\n","clf = RandomForestClassifier(**best_params)\n","\n","# Training the model on the training dataset\n","clf.fit(X_train, y_train)\n","\n","# performing predictions\n","y_pred = clf.predict(X_test)\n","accuracy=metrics.accuracy_score(y_test,y_pred)\n","report=classification_report(y_test,y_pred)\n","\n","print(\"Classification Report:\")\n","print(report)\n"]},{"cell_type":"markdown","metadata":{},"source":["### 20%"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["dirty_data = pd.read_csv('./dirty_data/dirty_data_20.csv')\n","dirty_data.info()"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["# Check distribution of target variable\n","y = dirty_data['Rating']\n","y_mean = dirty_data['Rating'].mean()\n","unos=0\n","zeros=0\n","for i in range(len(y)):\n","    if y[i] >= y_mean:\n","        y[i]=1\n","        unos+=1\n","    else:\n","        y[i]=0\n","        zeros+=1\n","\n","\n","sns.countplot(x = y)\n","print(f\"N={zeros} - Y={unos}\")\n","plt.show()"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["remove_missing(dirty_data)\n","remove_outliers(dirty_data)"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["# Split data into training and testing sets\n","X = dirty_data.drop(\"Rating\", axis=1)  # Features\n","y = encode_y(dirty_data)\n","\n","# Splitting dataset\n","X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2)\n","\n","# Building Model\n","best_params = {'max_depth': 4, 'max_features': 'sqrt', 'min_samples_leaf': 1, 'min_samples_split': 2, 'n_estimators': 100, 'random_state': 1000}\n","clf = RandomForestClassifier(**best_params)\n","\n","# Training the model on the training dataset\n","clf.fit(X_train, y_train)\n","\n","# performing predictions\n","y_pred = clf.predict(X_test)\n","accuracy=metrics.accuracy_score(y_test,y_pred)\n","report=classification_report(y_test,y_pred)\n","\n","print(\"Classification Report:\")\n","print(report)"]},{"cell_type":"markdown","metadata":{},"source":["### 30%"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["dirty_data = pd.read_csv('./dirty_data/dirty_data_30.csv')\n","dirty_data.info()"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["# Check distribution of target variable\n","y = dirty_data['Rating']\n","y_mean = dirty_data['Rating'].mean()\n","unos=0\n","zeros=0\n","for i in range(len(y)):\n","    if y[i] >= y_mean:\n","        y[i]=1\n","        unos+=1\n","    else:\n","        y[i]=0\n","        zeros+=1\n","\n","\n","sns.countplot(x = y)\n","print(f\"N={zeros} - Y={unos}\")\n","plt.show()"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["remove_missing(dirty_data)\n","remove_outliers(dirty_data)"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["# Split data into training and testing sets\n","X = dirty_data.drop(\"Rating\", axis=1)  # Features\n","y = encode_y(dirty_data)\n","\n","# Splitting dataset\n","X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2)\n","\n","# Building Model\n","best_params = {'max_depth': 4, 'max_features': 'sqrt', 'min_samples_leaf': 1, 'min_samples_split': 2, 'n_estimators': 100, 'random_state': 1000}\n","clf = RandomForestClassifier(**best_params)\n","\n","# Training the model on the training dataset\n","clf.fit(X_train, y_train)\n","\n","# performing predictions\n","y_pred = clf.predict(X_test)\n","accuracy=metrics.accuracy_score(y_test,y_pred)\n","report=classification_report(y_test,y_pred)\n","\n","print(\"Classification Report:\")\n","print(report)"]},{"cell_type":"markdown","metadata":{},"source":["## HoloClean\n","https://github.com/HoloClean/holoclean/tree/master\n","\n","Conda environment: holoclean37\n","\n","With Holoclean the results are stored as table in Postgresql, it also allow to evaluate result if you give it a clean dataset\n","\n","python holoclean/examples/holoclean_reapair_airbnb.py [database list]"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["def load_data(path):\n","    path = './clean_data/HoloClean/' + path\n","    return pd.read_csv(path)\n","\n","def get_differences(hc_data):\n","    # Differences between clean data and source data\n","    differences = data.count().sum() - (data == hc_data).astype(int).sum().sum()\n","    total_val = data.shape[0]*data.shape[1]\n","    print(f\"Different values: {differences} out of {total_val}\")\n","    print(f\"Percentage: {round((differences*100)/total_val, 2)}%\")\n","    print(f\"Null data:  {hc_data.isna().any().sum()}\")\n","\n","def manage_nan(hc_data, DELETE=False):\n","    # Replace NaN values\n","    if DELETE:\n","        hc_data.dropna(inplace=True)\n","\n","    else:\n","        for column in hc_data.columns:\n","            if hc_data[column].isnull().values.any():\n","                hc_data[column].fillna(float(hc_data[column].mean()), inplace=True)\n","\n","    print(f\"Null data:  {hc_data.isna().any().sum()}\")"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["def make_split(hc_data):\n","    # Encoding rating\n","    label_encoder = preprocessing.LabelEncoder() \n","    hc_data['Rating']= label_encoder.fit_transform(hc_data['Rating']) \n","\n","    # Split data into training and testing sets\n","    X = hc_data.drop(\"Rating\", axis=1)  # Features\n","    y = hc_data[\"Rating\"]  # Target variable\n","\n","    y_mean = hc_data['Rating'].mean()\n","    for i in range(len(y)):\n","        if y[i] >= y_mean:\n","            y[i]=1\n","        else:\n","            y[i]=0\n","\n","    # Splitting dataset\n","    return train_test_split(X, y, test_size=0.2)\n","\n","def train_model(X_train, X_test, y_train, y_test):\n","    # Building Model\n","    clf = RandomForestClassifier(**best_params)\n","\n","    # Training the model on the training dataset\n","    clf.fit(X_train, y_train)\n","\n","    # performing predictions\n","    y_pred = clf.predict(X_test)\n","    accuracy=metrics.accuracy_score(y_test,y_pred)\n","    report=classification_report(y_test,y_pred)\n","\n","    print(\"Classification Report:\")\n","    print(report)"]},{"cell_type":"markdown","metadata":{},"source":["### 10% Data"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["hc_data = load_data('dirty_data_10_repaired.csv')\n","hc_data.info()"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["# Check distribution of target variable\n","y = hc_data['Rating']\n","y_mean = hc_data['Rating'].mean()\n","unos=0\n","zeros=0\n","for i in range(len(y)):\n","    if y[i] >= y_mean:\n","        y[i]=1\n","        unos+=1\n","    else:\n","        y[i]=0\n","        zeros+=1\n","\n","\n","sns.countplot(x = y)\n","print(f\"N={zeros} - Y={unos}\")\n","plt.show()"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["get_differences(hc_data)"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["manage_nan(hc_data)"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["X_train, X_test, y_train, y_test = make_split(hc_data)\n","train_model(X_train, X_test, y_train, y_test)"]},{"cell_type":"markdown","metadata":{},"source":["### 20% Data"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["hc_data = load_data('dirty_data_20_repaired.csv')\n","hc_data.info()"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["# Check distribution of target variable\n","y = hc_data['Rating']\n","y_mean = hc_data['Rating'].mean()\n","unos=0\n","zeros=0\n","for i in range(len(y)):\n","    if y[i] >= y_mean:\n","        y[i]=1\n","        unos+=1\n","    else:\n","        y[i]=0\n","        zeros+=1\n","\n","\n","sns.countplot(x = y)\n","print(f\"N={zeros} - Y={unos}\")\n","plt.show()"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["get_differences(hc_data)"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["manage_nan(hc_data)"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["X_train, X_test, y_train, y_test = make_split(hc_data)\n","train_model(X_train, X_test, y_train, y_test)"]},{"cell_type":"markdown","metadata":{},"source":["### 30% Data"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["hc_data = load_data('dirty_data_30_repaired.csv')\n","hc_data.info()"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["# Check distribution of target variable\n","y = hc_data['Rating']\n","y_mean = hc_data['Rating'].mean()\n","unos=0\n","zeros=0\n","for i in range(len(y)):\n","    if y[i] >= y_mean:\n","        y[i]=1\n","        unos+=1\n","    else:\n","        y[i]=0\n","        zeros+=1\n","\n","\n","sns.countplot(x = y)\n","print(f\"N={zeros} - Y={unos}\")\n","plt.show()"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["get_differences(hc_data)"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["manage_nan(hc_data)"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["X_train, X_test, y_train, y_test = make_split(hc_data)\n","train_model(X_train, X_test, y_train, y_test)"]},{"cell_type":"markdown","metadata":{},"source":["## CleanML\n","https://github.com/chu-data-lab/CleanML\n","\n","Conda environment: CleanML\n","\n","Mst add dataset into databases with the present errors, in this case it is modified to do not run holoclean cleaning because we already did.\n","it has various method of cleaning listed and after clean the data it performs the machine learning task givin in output a json file with the results of the various experiments.\n","\n","The Random forest classifier is trained into CleanML directly, so we can analyze the result directly.\n","If we want to reproduce data we have the csv files for each run, so we find the best result under this section and load data\n","\n","python3 main.py --run_experiments"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["def get_best(path):\n","    path = './clean_data/CleanML/' + path\n","    cleanml = json.load(open(path))\n","\n","    best = []\n","    for item in cleanml:\n","        if best == []:\n","            best = item\n","\n","        if cleanml[item]['train_acc'] > cleanml[best]['train_acc']:\n","            best = item\n","\n","    print(best)\n","    print(json.dumps(cleanml[best], indent = 4, sort_keys=True))"]},{"cell_type":"markdown","metadata":{},"source":["### 10%"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["get_best('dirty_10_result.json')"]},{"cell_type":"markdown","metadata":{},"source":["### 20%"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["get_best('dirty_20_result.json')"]},{"cell_type":"markdown","metadata":{},"source":["### 30%"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["get_best('dirty_30_result.json')"]},{"cell_type":"markdown","metadata":{},"source":["### Reproduce"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["def load_data(path):\n","    path = './clean_data/CleanML/' + path\n","    return pd.read_csv(path)\n","\n","def get_differences(hc_data):\n","    # Differences between clean data and source data\n","    data=pd.read_csv('./source_data/airbnb_trim.csv')\n","\n","    differences = data[:hc_data.shape[1]].count().sum() - (data == hc_data).astype(int).sum().sum()\n","    total_val = hc_data.shape[0]*hc_data.shape[1]\n","    print(f\"Different values: {differences} out of {total_val}\")\n","    print(f\"Percentage: {round((differences*100)/total_val, 2)}%\")\n","    print(f\"Null data:  {hc_data.isna().any().sum()}\")\n","\n","def manage_nan(hc_data, DELETE=False):\n","    # Replace NaN values\n","    if DELETE:\n","        hc_data.dropna(inplace=True)\n","\n","    else:\n","        for column in hc_data.columns:\n","            if hc_data[column].isnull().values.any():\n","                hc_data[column].fillna(float(hc_data[column].mean()), inplace=True)\n","\n","    print(f\"Null data:  {hc_data.isna().any().sum()}\")"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["def encode_y(data):\n","    label_encoder = preprocessing.LabelEncoder() \n","    data['Rating']= label_encoder.fit_transform(data['Rating']) \n","\n","    y = data['Rating']\n","    y_mean = data['Rating'].mean()\n","    for i in range(len(y)):\n","        if y[i] >= y_mean:\n","            y[i]=1\n","        else:\n","            y[i]=0\n","\n","    return y\n","\n","def train_model(X_train, X_test, y_train, y_test):\n","    # Building Model\n","    clf = RandomForestClassifier(**best_params)\n","\n","    # Training the model on the training dataset\n","    clf.fit(X_train, y_train)\n","\n","    # performing predictions\n","    y_pred = clf.predict(X_test)\n","    accuracy=metrics.accuracy_score(y_test,y_pred)\n","    report=classification_report(y_test,y_pred)\n","\n","    print(\"Classification Report:\")\n","    print(report)"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["train = load_data('dirty_10/outliers/clean_IQR_impute_mode_dummy_train.csv')\n","test = load_data('dirty_10/outliers/clean_IQR_impute_mode_dummy_test.csv')\n","dataset = pd.concat([train, test], ignore_index=True)\n","dataset.info()"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["# Check distribution of target variable\n","y = dataset['Rating']\n","y_mean = dataset['Rating'].mean()\n","unos=0\n","zeros=0\n","for i in range(len(y)):\n","    if y[i] >= y_mean:\n","        y[i]=1\n","        unos+=1\n","    else:\n","        y[i]=0\n","        zeros+=1\n","\n","\n","sns.countplot(x = y)\n","print(f\"N={zeros} - Y={unos}\")\n","plt.show()"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["X_train = train.drop(\"Rating\", axis=1)\n","X_test = test.drop(\"Rating\", axis=1)\n","\n","y_train = encode_y(train)\n","y_test = encode_y(test)\n","\n","train_model(X_train, X_test, y_train, y_test)"]},{"cell_type":"markdown","metadata":{},"source":["## AutoClean\n","https://github.com/elisemercury/AutoClean\n","\n","Python package to autoclean, easy to use and updated"]},{"cell_type":"markdown","metadata":{},"source":["### 10%"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["data = pd.read_csv('./dirty_data/dirty_data_10.csv')\n","pipeline = AutoClean(data, missing_num='mean')\n","\n","dataset = pipeline.output\n","dataset.to_csv('./clean_data/AutoClean/dirty_10_clean.csv', index=False, sep=',')\n","dataset.info()"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["# Check distribution of target variable\n","y = dataset['Rating']\n","y_mean = dataset['Rating'].mean()\n","unos=0\n","zeros=0\n","for i in range(len(y)):\n","    if y[i] >= y_mean:\n","        y[i]=1\n","        unos+=1\n","    else:\n","        y[i]=0\n","        zeros+=1\n","\n","\n","sns.countplot(x = y)\n","print(f\"N={zeros} - Y={unos}\")\n","plt.show()"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["get_differences(dataset)"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["X_train, X_test, y_train, y_test = make_split(dataset)\n","train_model(X_train, X_test, y_train, y_test)"]},{"cell_type":"markdown","metadata":{},"source":["### 20%"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["data = pd.read_csv('./dirty_data/dirty_data_20.csv')\n","pipeline = AutoClean(data, missing_num='mean')\n","\n","dataset = pipeline.output\n","dataset.to_csv('./clean_data/AutoClean/dirty_20_clean.csv', index=False, sep=',')\n","dataset.info()"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["# Check distribution of target variable\n","y = dataset['Rating']\n","y_mean = dataset['Rating'].mean()\n","unos=0\n","zeros=0\n","for i in range(len(y)):\n","    if y[i] >= y_mean:\n","        y[i]=1\n","        unos+=1\n","    else:\n","        y[i]=0\n","        zeros+=1\n","\n","\n","sns.countplot(x = y)\n","print(f\"N={zeros} - Y={unos}\")\n","plt.show()"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["# Differences between clean data and source data\n","hc_data=pd.read_csv('./source_data/airbnb_trim.csv')\n","\n","differences = data[:hc_data.shape[1]].count().sum() - (data == hc_data).astype(int).sum().sum()\n","total_val = hc_data.shape[0]*hc_data.shape[1]\n","print(f\"Different values: {differences} out of {total_val}\")\n","print(f\"Percentage: {round((differences*100)/total_val, 2)}%\")\n","print(f\"Null data:  {hc_data.isna().any().sum()}\")"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["X_train, X_test, y_train, y_test = make_split(dataset)\n","train_model(X_train, X_test, y_train, y_test)"]},{"cell_type":"markdown","metadata":{},"source":["### 30%"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["data = pd.read_csv('./dirty_data/dirty_data_30.csv')\n","pipeline = AutoClean(data, missing_num='mean')\n","\n","dataset = pipeline.output\n","dataset.to_csv('./clean_data/AutoClean/dirty_30_clean.csv', index=False, sep=',')\n","dataset.info()"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["# Check distribution of target variable\n","y = dataset['Rating']\n","y_mean = dataset['Rating'].mean()\n","unos=0\n","zeros=0\n","for i in range(len(y)):\n","    if y[i] >= y_mean:\n","        y[i]=1\n","        unos+=1\n","    else:\n","        y[i]=0\n","        zeros+=1\n","\n","\n","sns.countplot(x = y)\n","print(f\"N={zeros} - Y={unos}\")\n","plt.show()"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["# Differences between clean data and source data\n","hc_data=pd.read_csv('./source_data/airbnb_trim.csv')\n","\n","differences = data[:hc_data.shape[1]].count().sum() - (data == hc_data).astype(int).sum().sum()\n","total_val = hc_data.shape[0]*hc_data.shape[1]\n","print(f\"Different values: {differences} out of {total_val}\")\n","print(f\"Percentage: {round((differences*100)/total_val, 2)}%\")\n","print(f\"Null data:  {hc_data.isna().any().sum()}\")"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["X_train, X_test, y_train, y_test = make_split(dataset)\n","train_model(X_train, X_test, y_train, y_test)"]}],"metadata":{"colab":{"authorship_tag":"ABX9TyNAsN5OwFnWz+OA0YDypwiA","provenance":[]},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.8.8"}},"nbformat":4,"nbformat_minor":0}
